{
	"name": "SouthDataMigrate",
	"properties": {
		"type": "MappingDataFlow",
		"typeProperties": {
			"sources": [
				{
					"dataset": {
						"referenceName": "SouthSource",
						"type": "DatasetReference"
					},
					"name": "ReadSource"
				},
				{
					"dataset": {
						"referenceName": "DatavalidationSegment",
						"type": "DatasetReference"
					},
					"name": "ReadSegmentAllowed"
				},
				{
					"dataset": {
						"referenceName": "DatavalidationShipMode",
						"type": "DatasetReference"
					},
					"name": "ReadShipModeAllowed"
				}
			],
			"sinks": [
				{
					"dataset": {
						"referenceName": "SalesSQL",
						"type": "DatasetReference"
					},
					"name": "InsertinSQL",
					"rejectedDataLinkedService": {
						"referenceName": "AzureBlobStorage1",
						"type": "LinkedServiceReference"
					}
				},
				{
					"dataset": {
						"referenceName": "DumpRowstobeFixed",
						"type": "DatasetReference"
					},
					"name": "InsertinCSV"
				},
				{
					"dataset": {
						"referenceName": "SalesLogsSQL",
						"type": "DatasetReference"
					},
					"name": "AddLogs"
				}
			],
			"transformations": [
				{
					"name": "RemoveDuplicates"
				},
				{
					"name": "RowswithNulls"
				},
				{
					"name": "AddColumns"
				},
				{
					"name": "SourceRowCount"
				},
				{
					"name": "ErrorRowCounts"
				},
				{
					"name": "DuplicateRowCounts"
				},
				{
					"name": "AddDuplicateRows"
				},
				{
					"name": "AddErrorRoes"
				},
				{
					"name": "FlattenandRenameColumns"
				},
				{
					"name": "MergeSegment"
				},
				{
					"name": "MergeShipMode"
				}
			],
			"scriptLines": [
				"source(output(",
				"          sample_data_south as (Order as string, Date as string, {Date Shipped} as string, {Shipping Method} as string, {Customer Number} as string, Segment as string, {Postal Code 1} as string, {Product ID} as string, Sales as string, Quantity as string, Discount as string, Profit as string)[]",
				"     ),",
				"     allowSchemaDrift: false,",
				"     validateSchema: true,",
				"     ignoreNoFilesFound: false,",
				"     rowUrlColumn: 'Source_File',",
				"     documentForm: 'documentPerLine',",
				"     mode: 'read') ~> ReadSource",
				"source(output(",
				"          {Segment Allowed} as string",
				"     ),",
				"     allowSchemaDrift: false,",
				"     validateSchema: true,",
				"     ignoreNoFilesFound: false) ~> ReadSegmentAllowed",
				"source(output(",
				"          {Ship Mode Allowed} as string",
				"     ),",
				"     allowSchemaDrift: false,",
				"     validateSchema: true,",
				"     ignoreNoFilesFound: false) ~> ReadShipModeAllowed",
				"RowswithNulls@Retain aggregate(groupBy({Order ID},",
				"          {Ship Mode Allowed},",
				"          {Customer ID},",
				"          {Segment Allowed},",
				"          {Product ID},",
				"          Source_File,",
				"          TimeStamp,",
				"          {Order Date Converted},",
				"          {Ship Date Converted},",
				"          {Postal Code Converted},",
				"          {Sales Converted},",
				"          {Quantity Converted},",
				"          {Discount Converted},",
				"          {Profit Converted}),",
				"     Dummy = sum(1)) ~> RemoveDuplicates",
				"AddColumns split(!isNull({Order ID}) && !isNull({Order Date Converted}) && !isNull({Ship Date Converted}) && !isNull({Ship Mode Allowed}) && !isNull({Customer ID}) && !isNull({Segment Allowed}) && !isNull({Postal Code Converted}) && !isNull({Product ID}) && !isNull({Sales Converted}) && !isNull({Quantity Converted}) && !isNull({Discount Converted}) && !isNull({Profit Converted}),",
				"     disjoint: false) ~> RowswithNulls@(Retain, Ignore)",
				"MergeShipMode derive(TimeStamp = currentUTC(),",
				"          {Order Date Converted} = iif(length({Order Date}) ==9,toDate(replace({Order Date}, '202', '/2'),'dd/MM/yy'),toDate({Order Date} ,'MM/dd/yy')),",
				"          {Ship Date Converted} = toDate({Ship Date},'MM/dd/yy'),",
				"          {Postal Code Converted} = toInteger({Postal Code}),",
				"          {Sales Converted} = toDouble(Sales),",
				"          {Quantity Converted} = toInteger(Quantity),",
				"          {Discount Converted} = round(toFloat(Discount),2),",
				"          {Profit Converted} = round(toFloat(regexReplace(Profit,'[^.0-9]','')),2)) ~> AddColumns",
				"AddColumns aggregate(groupBy(Source_File,",
				"          TimeStamp),",
				"     RowCount = sum(1)) ~> SourceRowCount",
				"RowswithNulls@Ignore aggregate(groupBy(Source_File,",
				"          TimeStamp),",
				"     ErrorRowCount = sum(1)) ~> ErrorRowCounts",
				"RemoveDuplicates aggregate(groupBy(Source_File,",
				"          TimeStamp),",
				"     DuplicateRowCount = sum(iif(Dummy != 1, Dummy - 1, toLong(0)))) ~> DuplicateRowCounts",
				"SourceRowCount, DuplicateRowCounts join(SourceRowCount@Source_File == DuplicateRowCounts@Source_File,",
				"     joinType:'left',",
				"     matchType:'exact',",
				"     ignoreSpaces: false,",
				"     broadcast: 'auto')~> AddDuplicateRows",
				"AddDuplicateRows, ErrorRowCounts join(SourceRowCount@Source_File == ErrorRowCounts@Source_File,",
				"     joinType:'left',",
				"     matchType:'exact',",
				"     ignoreSpaces: false,",
				"     broadcast: 'auto')~> AddErrorRoes",
				"ReadSource foldDown(unroll(sample_data_south),",
				"     mapColumn(",
				"          {Order ID} = sample_data_south.Order,",
				"          {Order Date} = sample_data_south.Date,",
				"          {Ship Date} = sample_data_south.{Date Shipped},",
				"          {Ship Mode} = sample_data_south.{Shipping Method},",
				"          {Customer ID} = sample_data_south.{Customer Number},",
				"          Segment = sample_data_south.Segment,",
				"          {Postal Code} = sample_data_south.{Postal Code 1},",
				"          {Product ID} = sample_data_south.{Product ID},",
				"          Sales = sample_data_south.Sales,",
				"          Quantity = sample_data_south.Quantity,",
				"          Discount = sample_data_south.Discount,",
				"          Profit = sample_data_south.Profit,",
				"          Source_File",
				"     ),",
				"     skipDuplicateMapInputs: false,",
				"     skipDuplicateMapOutputs: false) ~> FlattenandRenameColumns",
				"FlattenandRenameColumns, ReadSegmentAllowed join(fuzzyCompare(Segment, {Segment Allowed}, 85.00),",
				"     joinType:'left',",
				"     matchType:'fuzzy',",
				"     ignoreSpaces: true,",
				"     broadcast: 'off')~> MergeSegment",
				"MergeSegment, ReadShipModeAllowed join(fuzzyCompare({Ship Mode}, {Ship Mode Allowed}, 85.00),",
				"     joinType:'left',",
				"     matchType:'fuzzy',",
				"     ignoreSpaces: true,",
				"     broadcast: 'auto')~> MergeShipMode",
				"RemoveDuplicates sink(allowSchemaDrift: false,",
				"     validateSchema: true,",
				"     input(",
				"          Order_ID as string,",
				"          Order_Date as date,",
				"          Ship_Date as date,",
				"          Ship_Mode as string,",
				"          Customer_ID as string,",
				"          Segment as string,",
				"          Postal_Code as integer,",
				"          Product_ID as string,",
				"          SALES as decimal(20,2),",
				"          QUANTITY as integer,",
				"          Discount as double,",
				"          Profit as double,",
				"          Create_Timestamp as timestamp,",
				"          SOURCE_FILE as string",
				"     ),",
				"     deletable:false,",
				"     insertable:true,",
				"     updateable:false,",
				"     upsertable:false,",
				"     format: 'table',",
				"     skipDuplicateMapInputs: true,",
				"     skipDuplicateMapOutputs: true,",
				"     errorHandlingOption: 'allErrors',",
				"     outputRejectedData: true,",
				"     rejectedData_container: 'tes-rawdata',",
				"     rejectedData_folderPath: 'Rows_to_be_Fixed',",
				"     transactionCommit: 'single',",
				"     reportSuccessOnError: false,",
				"     mapColumn(",
				"          Order_ID = {Order ID},",
				"          Order_Date = {Order Date Converted},",
				"          Ship_Date = {Ship Date Converted},",
				"          Ship_Mode = {Ship Mode Allowed},",
				"          Customer_ID = {Customer ID},",
				"          Segment = {Segment Allowed},",
				"          Postal_Code = {Postal Code Converted},",
				"          Product_ID = {Product ID},",
				"          SALES = {Sales Converted},",
				"          QUANTITY = {Quantity Converted},",
				"          Discount = {Discount Converted},",
				"          Profit = {Profit Converted},",
				"          Create_Timestamp = TimeStamp,",
				"          SOURCE_FILE = Source_File",
				"     )) ~> InsertinSQL",
				"RowswithNulls@Ignore sink(allowSchemaDrift: false,",
				"     validateSchema: true,",
				"     input(",
				"          Column_1 as string,",
				"          Column_2 as string,",
				"          Column_3 as string,",
				"          Column_4 as string,",
				"          Column_5 as string,",
				"          Column_6 as string,",
				"          Column_7 as string,",
				"          Column_8 as string,",
				"          Column_9 as string,",
				"          Column_10 as string,",
				"          Column_11 as string,",
				"          Column_12 as string",
				"     ),",
				"     skipDuplicateMapInputs: true,",
				"     skipDuplicateMapOutputs: true) ~> InsertinCSV",
				"AddErrorRoes sink(allowSchemaDrift: false,",
				"     validateSchema: true,",
				"     input(",
				"          SOURCE_FILE as string,",
				"          TIMESTAMP as timestamp,",
				"          SOURCE_ROW_COUNT as integer,",
				"          ERROR_ROWS as integer,",
				"          DUPLICATE_ROW_COUNT as integer,",
				"          FINAL_COUNT as integer",
				"     ),",
				"     deletable:false,",
				"     insertable:true,",
				"     updateable:false,",
				"     upsertable:false,",
				"     format: 'table',",
				"     skipDuplicateMapInputs: true,",
				"     skipDuplicateMapOutputs: true,",
				"     errorHandlingOption: 'stopOnFirstError',",
				"     mapColumn(",
				"          SOURCE_FILE = SourceRowCount@Source_File,",
				"          TIMESTAMP = SourceRowCount@TimeStamp,",
				"          SOURCE_ROW_COUNT = RowCount,",
				"          ERROR_ROWS = DuplicateRowCount,",
				"          DUPLICATE_ROW_COUNT = ErrorRowCount",
				"     )) ~> AddLogs"
			]
		}
	}
}