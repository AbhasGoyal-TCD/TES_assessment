{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"factoryName": {
			"type": "string",
			"metadata": "Data Factory name",
			"defaultValue": "Agoyal-factory"
		}
	},
	"variables": {
		"factoryId": "[concat('Microsoft.DataFactory/factories/', parameters('factoryName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('factoryName'), '/PostalCodesSQL')]",
			"type": "Microsoft.DataFactory/factories/datasets",
			"apiVersion": "2018-06-01",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureSqlDatabase1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [
					{
						"name": "Postal_Code",
						"type": "int",
						"precision": 10
					},
					{
						"name": "State",
						"type": "nvarchar"
					},
					{
						"name": "Region",
						"type": "nvarchar"
					},
					{
						"name": "Country_Region",
						"type": "varchar"
					},
					{
						"name": "Country",
						"type": "nvarchar"
					}
				],
				"typeProperties": {
					"schema": "dbo",
					"table": "Postal_Code"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/ProductTypesExcel')]",
			"type": "Microsoft.DataFactory/factories/datasets",
			"apiVersion": "2018-06-01",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureBlobStorage1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Excel",
				"typeProperties": {
					"sheetName": "product_types",
					"location": {
						"type": "AzureBlobStorageLocation",
						"fileName": "group_reference_data.xlsx",
						"container": "tesmetadata"
					},
					"firstRowAsHeader": true
				},
				"schema": [
					{
						"name": "product_type",
						"type": "String"
					},
					{
						"name": "product_category",
						"type": "String"
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/ProductTypesSQL')]",
			"type": "Microsoft.DataFactory/factories/datasets",
			"apiVersion": "2018-06-01",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureSqlDatabase1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [
					{
						"name": "Product_Type",
						"type": "nvarchar"
					},
					{
						"name": "Product_Category",
						"type": "nvarchar"
					}
				],
				"typeProperties": {
					"schema": "dbo",
					"table": "Product_Types"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/ProductsExcel')]",
			"type": "Microsoft.DataFactory/factories/datasets",
			"apiVersion": "2018-06-01",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureBlobStorage1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Excel",
				"typeProperties": {
					"sheetName": "products",
					"location": {
						"type": "AzureBlobStorageLocation",
						"fileName": "group_reference_data.xlsx",
						"container": "tesmetadata"
					},
					"firstRowAsHeader": true
				},
				"schema": [
					{
						"name": "product_id",
						"type": "String"
					},
					{
						"name": "product_name",
						"type": "String"
					},
					{
						"name": "category",
						"type": "String"
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/ProductsSQL')]",
			"type": "Microsoft.DataFactory/factories/datasets",
			"apiVersion": "2018-06-01",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureSqlDatabase1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [
					{
						"name": "Product_ID",
						"type": "nvarchar"
					},
					{
						"name": "Product_Name",
						"type": "nvarchar"
					},
					{
						"name": "Category",
						"type": "nvarchar"
					}
				],
				"typeProperties": {
					"schema": "dbo",
					"table": "Products"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/RowstobeFixed')]",
			"type": "Microsoft.DataFactory/factories/datasets",
			"apiVersion": "2018-06-01",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureBlobStorage1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Excel",
				"typeProperties": {
					"sheetName": "Sheet1",
					"location": {
						"type": "AzureBlobStorageLocation",
						"fileName": "Error_Rows.xlsx",
						"folderPath": "Rows_to_be_Fixed",
						"container": "tes-rawdata"
					},
					"firstRowAsHeader": true
				},
				"schema": [
					{
						"name": "Order_ID",
						"type": "String"
					},
					{
						"name": "Order_date",
						"type": "String"
					},
					{
						"name": "Ship_date",
						"type": "String"
					},
					{
						"name": "Ship_Mode",
						"type": "String"
					},
					{
						"name": "Customer_ID",
						"type": "String"
					},
					{
						"name": "Segment",
						"type": "String"
					},
					{
						"name": "Postal_Code",
						"type": "String"
					},
					{
						"name": "Product ID",
						"type": "String"
					},
					{
						"name": "Sales",
						"type": "String"
					},
					{
						"name": "Quantity",
						"type": "String"
					},
					{
						"name": "Discount",
						"type": "String"
					},
					{
						"name": "Profit",
						"type": "String"
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/SalesLogsSQL')]",
			"type": "Microsoft.DataFactory/factories/datasets",
			"apiVersion": "2018-06-01",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureSqlDatabase1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [
					{
						"name": "SOURCE_FILE",
						"type": "nvarchar"
					},
					{
						"name": "TIMESTAMP",
						"type": "datetime",
						"precision": 23,
						"scale": 3
					},
					{
						"name": "SOURCE_ROW_COUNT",
						"type": "int",
						"precision": 10
					},
					{
						"name": "ERROR_ROWS",
						"type": "int",
						"precision": 10
					},
					{
						"name": "DUPLICATE_ROW_COUNT",
						"type": "int",
						"precision": 10
					},
					{
						"name": "FINAL_COUNT",
						"type": "int",
						"precision": 10
					}
				],
				"typeProperties": {
					"schema": "dbo",
					"table": "MERGED_DATA_LOGS"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/SalesSQL')]",
			"type": "Microsoft.DataFactory/factories/datasets",
			"apiVersion": "2018-06-01",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureSqlDatabase1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [
					{
						"name": "Order_ID",
						"type": "nvarchar"
					},
					{
						"name": "Order_Date",
						"type": "date"
					},
					{
						"name": "Ship_Date",
						"type": "date"
					},
					{
						"name": "Ship_Mode",
						"type": "nvarchar"
					},
					{
						"name": "Customer_ID",
						"type": "nvarchar"
					},
					{
						"name": "Segment",
						"type": "nvarchar"
					},
					{
						"name": "Postal_Code",
						"type": "int",
						"precision": 10
					},
					{
						"name": "Product_ID",
						"type": "nvarchar"
					},
					{
						"name": "SALES",
						"type": "decimal",
						"precision": 20,
						"scale": 2
					},
					{
						"name": "QUANTITY",
						"type": "int",
						"precision": 10
					},
					{
						"name": "Discount",
						"type": "float",
						"precision": 15
					},
					{
						"name": "Profit",
						"type": "float",
						"precision": 15
					},
					{
						"name": "Create_Timestamp",
						"type": "datetime",
						"precision": 23,
						"scale": 3
					},
					{
						"name": "SOURCE_FILE",
						"type": "nvarchar"
					}
				],
				"typeProperties": {
					"schema": "dbo",
					"table": "MERGED_DATA"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/SourceDataset_lpx')]",
			"type": "Microsoft.DataFactory/factories/datasets",
			"apiVersion": "2018-06-01",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureBlobStorage1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Excel",
				"typeProperties": {
					"location": {
						"type": "AzureBlobStorageLocation",
						"folderPath": "Xlsx_Format1",
						"container": "tes-rawdata"
					},
					"sheetIndex": 0,
					"firstRowAsHeader": true
				},
				"schema": [
					{
						"name": "Order ID",
						"type": "String"
					},
					{
						"name": "Order Date",
						"type": "String"
					},
					{
						"name": "Ship Date",
						"type": "String"
					},
					{
						"name": "Ship Mode",
						"type": "String"
					},
					{
						"name": "Customer ID",
						"type": "String"
					},
					{
						"name": "Segment",
						"type": "String"
					},
					{
						"name": "Postal Code",
						"type": "String"
					},
					{
						"name": "Product ID",
						"type": "String"
					},
					{
						"name": "Sales",
						"type": "String"
					},
					{
						"name": "Quantity",
						"type": "String"
					},
					{
						"name": "Discount",
						"type": "String"
					},
					{
						"name": "Profit",
						"type": "String"
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/SouthSource')]",
			"type": "Microsoft.DataFactory/factories/datasets",
			"apiVersion": "2018-06-01",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureBlobStorage1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Json",
				"typeProperties": {
					"location": {
						"type": "AzureBlobStorageLocation",
						"folderPath": "South_Data",
						"container": "tes-rawdata"
					},
					"encodingName": "UTF-8"
				},
				"schema": {
					"type": "object",
					"properties": {
						"sample_data_south": {
							"type": "array",
							"items": {
								"type": "object",
								"properties": {
									"Order": {
										"type": "string"
									},
									"Date": {
										"type": "string"
									},
									"Date Shipped": {
										"type": "string"
									},
									"Shipping Method": {
										"type": "string"
									},
									"Customer Number": {
										"type": "string"
									},
									"Segment": {
										"type": "string"
									},
									"Postal Code 1": {
										"type": "string"
									},
									"Product ID": {
										"type": "string"
									},
									"Sales": {
										"type": "string"
									},
									"Quantity": {
										"type": "string"
									},
									"Discount": {
										"type": "string"
									},
									"Profit": {
										"type": "string"
									}
								}
							}
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/WestSource')]",
			"type": "Microsoft.DataFactory/factories/datasets",
			"apiVersion": "2018-06-01",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureBlobStorage1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Excel",
				"typeProperties": {
					"location": {
						"type": "AzureBlobStorageLocation",
						"folderPath": "West_Data",
						"container": "tes-rawdata"
					},
					"sheetIndex": 0
				},
				"schema": [
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/CentralDataMigrate')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "CentralSource",
								"type": "DatasetReference"
							},
							"name": "ReadSource"
						},
						{
							"dataset": {
								"referenceName": "DatavalidationSegment",
								"type": "DatasetReference"
							},
							"name": "ReadSegmentAllowed"
						},
						{
							"dataset": {
								"referenceName": "DatavalidationShipMode",
								"type": "DatasetReference"
							},
							"name": "ReadShipModeAllowed"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "SalesSQL",
								"type": "DatasetReference"
							},
							"name": "InsertinSQL",
							"rejectedDataLinkedService": {
								"referenceName": "AzureBlobStorage1",
								"type": "LinkedServiceReference"
							}
						},
						{
							"dataset": {
								"referenceName": "DumpRowstobeFixed",
								"type": "DatasetReference"
							},
							"name": "InsertinCSV"
						},
						{
							"dataset": {
								"referenceName": "SalesLogsSQL",
								"type": "DatasetReference"
							},
							"name": "AddLogs1"
						}
					],
					"transformations": [
						{
							"name": "RemoveDuplicates"
						},
						{
							"name": "RowswithNulls"
						},
						{
							"name": "AddColumns"
						},
						{
							"name": "SourceRowCount"
						},
						{
							"name": "ErrorRowCount"
						},
						{
							"name": "DuplicateRowCount"
						},
						{
							"name": "RenameColumns"
						},
						{
							"name": "AddDuplicateRows"
						},
						{
							"name": "AddErrorRows"
						},
						{
							"name": "MergeSegment"
						},
						{
							"name": "MergeShipMode"
						},
						{
							"name": "FixNullValues"
						}
					],
					"scriptLines": [
						"source(output(",
						"          {Order ID} as string,",
						"          {Order Date} as string,",
						"          {Ship Date} as string,",
						"          {Ship Mode} as string,",
						"          {Customer ID} as string,",
						"          Segment as string,",
						"          {Postal Code} as string,",
						"          {Product ID} as string,",
						"          Sales as string,",
						"          Quantity as string,",
						"          Discount as string,",
						"          Profit as string",
						"     ),",
						"     allowSchemaDrift: false,",
						"     validateSchema: true,",
						"     ignoreNoFilesFound: false,",
						"     rowUrlColumn: 'Source_File',",
						"     wildcardPaths:['Central_Data/*.xlsx'],",
						"     mode: 'read') ~> ReadSource",
						"source(output(",
						"          {Segment Allowed} as string",
						"     ),",
						"     allowSchemaDrift: false,",
						"     validateSchema: true,",
						"     ignoreNoFilesFound: false) ~> ReadSegmentAllowed",
						"source(output(",
						"          {Ship Mode Allowed} as string",
						"     ),",
						"     allowSchemaDrift: false,",
						"     validateSchema: true,",
						"     ignoreNoFilesFound: false) ~> ReadShipModeAllowed",
						"RowswithNulls@Retain aggregate(groupBy({Order ID},",
						"          {Ship Mode Allowed},",
						"          {Customer ID},",
						"          {Segment Allowed},",
						"          {Product ID},",
						"          Source_File,",
						"          TimeStamp,",
						"          {Order Date Converted},",
						"          {Ship Date Converted},",
						"          {Postal Code Converted},",
						"          {Sales Converted},",
						"          {Quantity Converted},",
						"          {Discount Converted},",
						"          {Profit Converted}),",
						"     Dummy = sum(1)) ~> RemoveDuplicates",
						"AddColumns split(!isNull({Order ID}) && !isNull({Order Date Converted}) && !isNull({Ship Date Converted}) && !isNull({Ship Mode Allowed}) && !isNull({Customer ID}) && !isNull({Segment Allowed}) && !isNull({Postal Code Converted}) && !isNull({Product ID}) && !isNull({Sales Converted}) && !isNull({Quantity Converted}) && !isNull({Discount Converted}) && !isNull({Profit Converted}),",
						"     disjoint: false) ~> RowswithNulls@(Retain, Ignore)",
						"MergeShipMode derive(TimeStamp = currentUTC(),",
						"          {Order Date Converted} = toDate({Order Date},'yyyy-MM-dd'),",
						"          {Ship Date Converted} = toDate({Ship Date},'yyyy-MM-dd'),",
						"          {Postal Code Converted} = toInteger({Postal Code}),",
						"          {Sales Converted} = toDouble(Sales),",
						"          {Quantity Converted} = toInteger(Quantity),",
						"          {Discount Converted} = round(toFloat(Discount),2),",
						"          {Profit Converted} = round(toFloat(Profit),2)) ~> AddColumns",
						"AddColumns aggregate(groupBy(Source_File,",
						"          TimeStamp),",
						"     RowCount = sum(1)) ~> SourceRowCount",
						"RowswithNulls@Ignore aggregate(groupBy(Source_File,",
						"          TimeStamp),",
						"     ErrorRowCount = sum(1)) ~> ErrorRowCount",
						"RemoveDuplicates aggregate(groupBy(Source_File,",
						"          TimeStamp),",
						"     DuplicateRowCount = sum(iif(Dummy != 1, Dummy - 1, toLong(0)))) ~> DuplicateRowCount",
						"ReadSource select(mapColumn(",
						"          {Order ID},",
						"          {Order Date},",
						"          {Ship Date},",
						"          {Ship Mode},",
						"          {Customer ID},",
						"          Segment,",
						"          {Postal Code},",
						"          {Product ID},",
						"          Sales,",
						"          Quantity,",
						"          Discount,",
						"          Profit,",
						"          Source_File",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> RenameColumns",
						"SourceRowCount, DuplicateRowCount join(SourceRowCount@Source_File == DuplicateRowCount@Source_File,",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> AddDuplicateRows",
						"AddDuplicateRows, ErrorRowCount join(SourceRowCount@Source_File == ErrorRowCount@Source_File,",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> AddErrorRows",
						"RenameColumns, ReadSegmentAllowed join(fuzzyCompare(Segment, {Segment Allowed}, 85.00),",
						"     joinType:'left',",
						"     matchType:'fuzzy',",
						"     ignoreSpaces: true,",
						"     broadcast: 'off')~> MergeSegment",
						"MergeSegment, ReadShipModeAllowed join(fuzzyCompare({Ship Mode}, {Ship Mode Allowed}, 85.00),",
						"     joinType:'left',",
						"     matchType:'fuzzy',",
						"     ignoreSpaces: true,",
						"     broadcast: 'off')~> MergeShipMode",
						"AddErrorRows derive(DuplicateRowCount = iif(isNull(DuplicateRowCount),0,toInteger(DuplicateRowCount)),",
						"          ErrorRowCount = iif(isNull(ErrorRowCount),0,toInteger(ErrorRowCount))) ~> FixNullValues",
						"RemoveDuplicates sink(allowSchemaDrift: false,",
						"     validateSchema: true,",
						"     input(",
						"          Order_ID as string,",
						"          Order_Date as date,",
						"          Ship_Date as date,",
						"          Ship_Mode as string,",
						"          Customer_ID as string,",
						"          Segment as string,",
						"          Postal_Code as integer,",
						"          Product_ID as string,",
						"          SALES as decimal(20,2),",
						"          QUANTITY as integer,",
						"          Discount as double,",
						"          Profit as double,",
						"          Create_Timestamp as timestamp,",
						"          SOURCE_FILE as string",
						"     ),",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'allErrors',",
						"     outputRejectedData: true,",
						"     rejectedData_container: 'tes-rawdata',",
						"     rejectedData_folderPath: 'Rows_to_be_Fixed',",
						"     transactionCommit: 'single',",
						"     reportSuccessOnError: false,",
						"     mapColumn(",
						"          Order_ID = {Order ID},",
						"          Order_Date = {Order Date Converted},",
						"          Ship_Date = {Ship Date Converted},",
						"          Ship_Mode = {Ship Mode Allowed},",
						"          Customer_ID = {Customer ID},",
						"          Segment = {Segment Allowed},",
						"          Postal_Code = {Postal Code Converted},",
						"          Product_ID = {Product ID},",
						"          SALES = {Sales Converted},",
						"          QUANTITY = {Quantity Converted},",
						"          Discount = {Discount Converted},",
						"          Profit = {Profit Converted},",
						"          Create_Timestamp = TimeStamp,",
						"          SOURCE_FILE = Source_File",
						"     )) ~> InsertinSQL",
						"RowswithNulls@Ignore sink(allowSchemaDrift: false,",
						"     validateSchema: true,",
						"     input(",
						"          Column_1 as string,",
						"          Column_2 as string,",
						"          Column_3 as string,",
						"          Column_4 as string,",
						"          Column_5 as string,",
						"          Column_6 as string,",
						"          Column_7 as string,",
						"          Column_8 as string,",
						"          Column_9 as string,",
						"          Column_10 as string,",
						"          Column_11 as string,",
						"          Column_12 as string",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> InsertinCSV",
						"FixNullValues sink(allowSchemaDrift: false,",
						"     validateSchema: true,",
						"     input(",
						"          SOURCE_FILE as string,",
						"          TIMESTAMP as timestamp,",
						"          SOURCE_ROW_COUNT as integer,",
						"          ERROR_ROWS as integer,",
						"          DUPLICATE_ROW_COUNT as integer,",
						"          FINAL_COUNT as integer",
						"     ),",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     mapColumn(",
						"          SOURCE_FILE = SourceRowCount@Source_File,",
						"          TIMESTAMP = SourceRowCount@TimeStamp,",
						"          SOURCE_ROW_COUNT = RowCount,",
						"          DUPLICATE_ROW_COUNT = DuplicateRowCount,",
						"          ERROR_ROWS = ErrorRowCount",
						"     )) ~> AddLogs1"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/datasets/SalesSQL')]",
				"[concat(variables('factoryId'), '/datasets/SalesLogsSQL')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/CompaniesMetadata')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "CompaniesExcel",
								"type": "DatasetReference"
							},
							"name": "ReadSource"
						},
						{
							"dataset": {
								"referenceName": "CompaniesSQL",
								"type": "DatasetReference"
							},
							"name": "ReadSQL"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "CompaniesSQL",
								"type": "DatasetReference"
							},
							"name": "InsertinSQL",
							"rejectedDataLinkedService": {
								"referenceName": "AzureBlobStorage1",
								"type": "LinkedServiceReference"
							}
						},
						{
							"dataset": {
								"referenceName": "DumpNullMetaRows",
								"type": "DatasetReference"
							},
							"name": "RecordNullRows"
						},
						{
							"dataset": {
								"referenceName": "MetaDataLogs",
								"type": "DatasetReference"
							},
							"name": "AddLogs"
						}
					],
					"transformations": [
						{
							"name": "CheckforDuplicates"
						},
						{
							"name": "CheckUpdate"
						},
						{
							"name": "MarkUpdates"
						},
						{
							"name": "TransformType"
						},
						{
							"name": "FilterExistingRows"
						},
						{
							"name": "SourceRowCount"
						},
						{
							"name": "DuplicateRowCount"
						},
						{
							"name": "ErrorRowCount"
						},
						{
							"name": "IgnoreRowCount"
						},
						{
							"name": "MergeDuplicateRows"
						},
						{
							"name": "MergeErrorRows"
						},
						{
							"name": "MergeIgnoreRows"
						},
						{
							"name": "FixNulls"
						},
						{
							"name": "AddSourceFile"
						}
					],
					"scriptLines": [
						"source(output(",
						"          company_id as string,",
						"          company as string,",
						"          contact as string,",
						"          region as string",
						"     ),",
						"     allowSchemaDrift: false,",
						"     validateSchema: true,",
						"     ignoreNoFilesFound: false) ~> ReadSource",
						"source(output(",
						"          Company_ID as integer,",
						"          Company as string,",
						"          Contact as string,",
						"          Region as string",
						"     ),",
						"     allowSchemaDrift: false,",
						"     validateSchema: true,",
						"     isolationLevel: 'READ_UNCOMMITTED',",
						"     format: 'table') ~> ReadSQL",
						"AddSourceFile aggregate(groupBy(company_id,",
						"          company,",
						"          contact,",
						"          region,",
						"          Source_File),",
						"     Dummy = sum(1)) ~> CheckforDuplicates",
						"TransformType, ReadSQL join(TransformType@company_id == ReadSQL@Company_ID,",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> CheckUpdate",
						"FilterExistingRows@Retain alterRow(updateIf(FilterExistingRows@Retain@company!=FilterExistingRows@Retain@Company||FilterExistingRows@Retain@contact!=FilterExistingRows@Retain@Contact||FilterExistingRows@Retain@region!=FilterExistingRows@Retain@Region)) ~> MarkUpdates",
						"CheckforDuplicates derive(company_id = toInteger(company_id)) ~> TransformType",
						"CheckUpdate split((isNull(ReadSQL@Company_ID) || (CheckforDuplicates@company != ReadSQL@Company || CheckforDuplicates@contact != ReadSQL@Contact || CheckforDuplicates@region != ReadSQL@Region)) \r",
						"&& (!isNull(TransformType@company_id) && !isNull(CheckforDuplicates@company) && !isNull(CheckforDuplicates@contact) && !isNull(CheckforDuplicates@region)),",
						"     isNull(ReadSQL@Company_ID) && (isNull(CheckforDuplicates@company) || isNull(CheckforDuplicates@contact) || isNull(CheckforDuplicates@region)),",
						"     disjoint: false) ~> FilterExistingRows@(Retain, FilterNullRows, Ignore)",
						"AddSourceFile aggregate(groupBy(Source_File),",
						"     SRowCount = sum(1)) ~> SourceRowCount",
						"CheckforDuplicates aggregate(groupBy(Source_File),",
						"     DuplicateRowCount = sum(iif(Dummy != 1, Dummy - 1, toLong(0)))) ~> DuplicateRowCount",
						"FilterExistingRows@FilterNullRows aggregate(groupBy(Source_File),",
						"     ERowcount = sum(1)) ~> ErrorRowCount",
						"FilterExistingRows@Ignore aggregate(groupBy(Source_File),",
						"     IRowCount = sum(1)) ~> IgnoreRowCount",
						"SourceRowCount, DuplicateRowCount join(SourceRowCount@Source_File == DuplicateRowCount@Source_File,",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> MergeDuplicateRows",
						"MergeDuplicateRows, ErrorRowCount join(SourceRowCount@Source_File == ErrorRowCount@Source_File,",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> MergeErrorRows",
						"MergeErrorRows, IgnoreRowCount join(SourceRowCount@Source_File == IgnoreRowCount@Source_File,",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> MergeIgnoreRows",
						"MergeIgnoreRows derive(DuplicateRowCount = iif(isNull(DuplicateRowCount),0,toInteger(DuplicateRowCount)),",
						"          ERowcount = iif(isNull(ERowcount),0,toInteger(ERowcount)),",
						"          IRowCount = iif(isNull(IRowCount),0,toInteger(IRowCount))) ~> FixNulls",
						"ReadSource derive(Source_File = 'Company') ~> AddSourceFile",
						"MarkUpdates sink(allowSchemaDrift: false,",
						"     validateSchema: true,",
						"     input(",
						"          Company_ID as integer,",
						"          Company as string,",
						"          Contact as string,",
						"          Region as string",
						"     ),",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:true,",
						"     upsertable:false,",
						"     keys:['Company_ID'],",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'allErrors',",
						"     outputRejectedData: true,",
						"     rejectedData_container: 'tesmetadata',",
						"     transactionCommit: 'single',",
						"     reportSuccessOnError: false,",
						"     mapColumn(",
						"          Company_ID = FilterExistingRows@Retain@company_id,",
						"          Company = FilterExistingRows@Retain@company,",
						"          Contact = FilterExistingRows@Retain@contact,",
						"          Region = FilterExistingRows@Retain@region",
						"     )) ~> InsertinSQL",
						"FilterExistingRows@FilterNullRows sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> RecordNullRows",
						"FixNulls sink(allowSchemaDrift: false,",
						"     validateSchema: true,",
						"     input(",
						"          SOURCE_FILE as string,",
						"          SOURCE_ROW_COUNT as integer,",
						"          ERROR_ROWS as integer,",
						"          DUPLICATE_ROW_COUNT as integer,",
						"          IGNORE_ROW_COUNT as integer,",
						"          FINAL_COUNT as integer",
						"     ),",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     mapColumn(",
						"          SOURCE_FILE = SourceRowCount@Source_File,",
						"          SOURCE_ROW_COUNT = SRowCount,",
						"          ERROR_ROWS = ERowcount,",
						"          DUPLICATE_ROW_COUNT = DuplicateRowCount,",
						"          IGNORE_ROW_COUNT = IRowCount",
						"     )) ~> AddLogs"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/EastDataMigrate')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "EastSource",
								"type": "DatasetReference"
							},
							"name": "ReadSource"
						},
						{
							"dataset": {
								"referenceName": "DatavalidationSegment",
								"type": "DatasetReference"
							},
							"name": "ReadSegmentAllowed"
						},
						{
							"dataset": {
								"referenceName": "DatavalidationShipMode",
								"type": "DatasetReference"
							},
							"name": "ReadShipModeAllowed"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "SalesSQL",
								"type": "DatasetReference"
							},
							"name": "InsertinSQL",
							"rejectedDataLinkedService": {
								"referenceName": "AzureBlobStorage1",
								"type": "LinkedServiceReference"
							}
						},
						{
							"dataset": {
								"referenceName": "DumpRowstobeFixed",
								"type": "DatasetReference"
							},
							"name": "InsertinCSV"
						},
						{
							"dataset": {
								"referenceName": "SalesLogsSQL",
								"type": "DatasetReference"
							},
							"name": "AddLogs"
						}
					],
					"transformations": [
						{
							"name": "RemoveDuplicates"
						},
						{
							"name": "RowswithNulls"
						},
						{
							"name": "AddColumns"
						},
						{
							"name": "SourceRowCount"
						},
						{
							"name": "ErrorRowCount"
						},
						{
							"name": "DuplicateRowCount"
						},
						{
							"name": "RenameColumns"
						},
						{
							"name": "AddDuplicateRows"
						},
						{
							"name": "AddErrorRoes"
						},
						{
							"name": "MergeSegment"
						},
						{
							"name": "MergeShipMode"
						},
						{
							"name": "FixNullValues"
						}
					],
					"scriptLines": [
						"source(output(",
						"          order_number as string,",
						"          ord_date as string,",
						"          ship_date as string,",
						"          ship_type as string,",
						"          customer_key as string,",
						"          customer_section as string,",
						"          post_code as string,",
						"          product_id as string,",
						"          sales as string,",
						"          {#_items} as string,",
						"          reduction as string,",
						"          total as string",
						"     ),",
						"     allowSchemaDrift: false,",
						"     validateSchema: true,",
						"     ignoreNoFilesFound: false,",
						"     rowUrlColumn: 'Source_File',",
						"     wildcardPaths:['East_Data/*.csv'],",
						"     mode: 'read') ~> ReadSource",
						"source(output(",
						"          {Segment Allowed} as string",
						"     ),",
						"     allowSchemaDrift: false,",
						"     validateSchema: true,",
						"     ignoreNoFilesFound: false) ~> ReadSegmentAllowed",
						"source(output(",
						"          {Ship Mode Allowed} as string",
						"     ),",
						"     allowSchemaDrift: false,",
						"     validateSchema: true,",
						"     ignoreNoFilesFound: false) ~> ReadShipModeAllowed",
						"RowswithNulls@Retain aggregate(groupBy({Order ID},",
						"          {Ship Mode Allowed},",
						"          {Customer ID},",
						"          {Segment Allowed},",
						"          {Product ID},",
						"          Source_File,",
						"          TimeStamp,",
						"          {Order Date Converted},",
						"          {Ship Date Converted},",
						"          {Postal Code Converted},",
						"          {Sales Converted},",
						"          {Quantity Converted},",
						"          {Discount Converted},",
						"          {Profit Converted}),",
						"     Dummy = sum(1)) ~> RemoveDuplicates",
						"AddColumns split(!isNull({Order ID}) && !isNull({Order Date Converted}) && !isNull({Ship Date Converted}) && !isNull({Ship Mode Allowed}) && !isNull({Customer ID}) && !isNull({Segment Allowed}) && !isNull({Postal Code Converted}) && !isNull({Postal Code}) && !isNull({Sales Converted}) && !isNull({Quantity Converted}) && !isNull({Discount Converted}) && !isNull({Profit Converted}),",
						"     disjoint: false) ~> RowswithNulls@(Retain, Ignore)",
						"MergeShipMode derive(TimeStamp = currentUTC(),",
						"          {Order Date Converted} = toDate(replace({Order Date},'31/11','30/11'),'dd/MM/yyyy'),",
						"          {Ship Date Converted} = toDate(iif(length({Ship Date}) == 9, replace({Ship Date},'202','/202'), replace({Ship Date},'31/06','30/06')),'dd/MM/yyyy'),",
						"          {Postal Code Converted} = toInteger({Postal Code}),",
						"          {Sales Converted} = toDouble(regexReplace(Sales,'[^.0-9]','')),",
						"          {Quantity Converted} = toInteger(regexReplace(Quantity,'[^.0-9]','')),",
						"          {Discount Converted} = round(toFloat(Discount),2),",
						"          {Profit Converted} = round(toFloat(Profit),2),",
						"          {Product ID} = replace({Product ID}, '*', '')) ~> AddColumns",
						"AddColumns aggregate(groupBy(Source_File,",
						"          TimeStamp),",
						"     RowCount = sum(1)) ~> SourceRowCount",
						"RowswithNulls@Ignore aggregate(groupBy(Source_File,",
						"          TimeStamp),",
						"     ErrorRowCount = sum(1)) ~> ErrorRowCount",
						"RemoveDuplicates aggregate(groupBy(Source_File,",
						"          TimeStamp),",
						"     DuplicateRowCount = sum(iif(Dummy != 1, Dummy - 1, toLong(0)))) ~> DuplicateRowCount",
						"ReadSource select(mapColumn(",
						"          {Order ID} = order_number,",
						"          {Order Date} = ord_date,",
						"          {Ship Date} = ship_date,",
						"          {Ship Mode} = ship_type,",
						"          {Customer ID} = customer_key,",
						"          Segment = customer_section,",
						"          {Postal Code} = post_code,",
						"          {Product ID} = product_id,",
						"          Sales = sales,",
						"          Quantity = {#_items},",
						"          Discount = reduction,",
						"          Profit = total,",
						"          Source_File",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> RenameColumns",
						"SourceRowCount, DuplicateRowCount join(SourceRowCount@Source_File == DuplicateRowCount@Source_File,",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> AddDuplicateRows",
						"AddDuplicateRows, ErrorRowCount join(SourceRowCount@Source_File == ErrorRowCount@Source_File,",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> AddErrorRoes",
						"RenameColumns, ReadSegmentAllowed join(fuzzyCompare(Segment, {Segment Allowed}, 85.00),",
						"     joinType:'left',",
						"     matchType:'fuzzy',",
						"     ignoreSpaces: true,",
						"     broadcast: 'off')~> MergeSegment",
						"MergeSegment, ReadShipModeAllowed join(fuzzyCompare({Ship Mode}, {Ship Mode Allowed}, 85.00),",
						"     joinType:'left',",
						"     matchType:'fuzzy',",
						"     ignoreSpaces: true,",
						"     broadcast: 'off')~> MergeShipMode",
						"AddErrorRoes derive(DuplicateRowCount = iif(isNull(DuplicateRowCount),0,toInteger(DuplicateRowCount)),",
						"          ErrorRowCount = iif(isNull(ErrorRowCount),0,toInteger(ErrorRowCount))) ~> FixNullValues",
						"RemoveDuplicates sink(allowSchemaDrift: false,",
						"     validateSchema: true,",
						"     input(",
						"          Order_ID as string,",
						"          Order_Date as date,",
						"          Ship_Date as date,",
						"          Ship_Mode as string,",
						"          Customer_ID as string,",
						"          Segment as string,",
						"          Postal_Code as integer,",
						"          Product_ID as string,",
						"          SALES as decimal(20,2),",
						"          QUANTITY as integer,",
						"          Discount as double,",
						"          Profit as double,",
						"          Create_Timestamp as timestamp,",
						"          SOURCE_FILE as string",
						"     ),",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'allErrors',",
						"     outputRejectedData: true,",
						"     rejectedData_container: 'tes-rawdata',",
						"     rejectedData_folderPath: 'Rows_to_be_Fixed',",
						"     transactionCommit: 'single',",
						"     reportSuccessOnError: true,",
						"     mapColumn(",
						"          Order_ID = {Order ID},",
						"          Order_Date = {Order Date Converted},",
						"          Ship_Date = {Ship Date Converted},",
						"          Ship_Mode = {Ship Mode Allowed},",
						"          Customer_ID = {Customer ID},",
						"          Segment = {Segment Allowed},",
						"          Postal_Code = {Postal Code Converted},",
						"          Product_ID = {Product ID},",
						"          SALES = {Sales Converted},",
						"          QUANTITY = {Quantity Converted},",
						"          Discount = {Discount Converted},",
						"          Profit = {Profit Converted},",
						"          Create_Timestamp = TimeStamp,",
						"          SOURCE_FILE = Source_File",
						"     )) ~> InsertinSQL",
						"RowswithNulls@Ignore sink(allowSchemaDrift: false,",
						"     validateSchema: true,",
						"     input(",
						"          Column_1 as string,",
						"          Column_2 as string,",
						"          Column_3 as string,",
						"          Column_4 as string,",
						"          Column_5 as string,",
						"          Column_6 as string,",
						"          Column_7 as string,",
						"          Column_8 as string,",
						"          Column_9 as string,",
						"          Column_10 as string,",
						"          Column_11 as string,",
						"          Column_12 as string",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> InsertinCSV",
						"FixNullValues sink(allowSchemaDrift: false,",
						"     validateSchema: true,",
						"     input(",
						"          SOURCE_FILE as string,",
						"          TIMESTAMP as timestamp,",
						"          SOURCE_ROW_COUNT as integer,",
						"          ERROR_ROWS as integer,",
						"          DUPLICATE_ROW_COUNT as integer,",
						"          FINAL_COUNT as integer",
						"     ),",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     mapColumn(",
						"          SOURCE_FILE = SourceRowCount@Source_File,",
						"          TIMESTAMP = SourceRowCount@TimeStamp,",
						"          SOURCE_ROW_COUNT = RowCount,",
						"          ERROR_ROWS = ErrorRowCount,",
						"          DUPLICATE_ROW_COUNT = DuplicateRowCount",
						"     )) ~> AddLogs"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/datasets/SalesSQL')]",
				"[concat(variables('factoryId'), '/datasets/SalesLogsSQL')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/LoadSouthSourceCount')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "SouthSource",
								"type": "DatasetReference"
							},
							"name": "Json"
						}
					],
					"sinks": [
						{
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "FlattenJson"
						},
						{
							"name": "aggregate1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          sample_data_south as (Order as string, Date as string, {Date Shipped} as string, {Shipping Method} as string, {Customer Number} as string, Segment as string, {Postal Code 1} as string, {Product ID} as string, Sales as string, Quantity as string, Discount as string, Profit as string)[]",
						"     ),",
						"     allowSchemaDrift: false,",
						"     validateSchema: true,",
						"     ignoreNoFilesFound: false,",
						"     rowUrlColumn: 'Source File',",
						"     documentForm: 'documentPerLine',",
						"     mode: 'read') ~> Json",
						"Json foldDown(unroll(sample_data_south),",
						"     mapColumn(",
						"          {Customer ID} = sample_data_south.{Customer Number},",
						"          {Order Date} = sample_data_south.Date,",
						"          {Ship Date} = sample_data_south.{Date Shipped},",
						"          Discount = sample_data_south.Discount,",
						"          {Order ID} = sample_data_south.Order,",
						"          {Postal Code} = sample_data_south.{Postal Code 1},",
						"          {Product ID} = sample_data_south.{Product ID},",
						"          Profit = sample_data_south.Profit,",
						"          Quantity = sample_data_south.Quantity,",
						"          Sales = sample_data_south.Sales,",
						"          Segment = sample_data_south.Segment,",
						"          {Ship Mode} = sample_data_south.{Shipping Method},",
						"          {Source File}",
						"     ),",
						"     skipDuplicateMapInputs: false,",
						"     skipDuplicateMapOutputs: false) ~> FlattenJson",
						"FlattenJson aggregate(groupBy({Source File}),",
						"     RowCount = sum(1)) ~> aggregate1",
						"aggregate1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     store: 'cache',",
						"     format: 'inline',",
						"     output: true,",
						"     saveOrder: 1) ~> sink1"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/datasets/SouthSource')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/PostalCodeMetadata_v2')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "PostalCodesExcel",
								"type": "DatasetReference"
							},
							"name": "ReadSource"
						},
						{
							"dataset": {
								"referenceName": "PostalCodesSQL",
								"type": "DatasetReference"
							},
							"name": "ReadSQL"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "PostalCodesSQL",
								"type": "DatasetReference"
							},
							"name": "InsertinSQL",
							"rejectedDataLinkedService": {
								"referenceName": "AzureBlobStorage1",
								"type": "LinkedServiceReference"
							}
						},
						{
							"dataset": {
								"referenceName": "DumpNullMetaRows",
								"type": "DatasetReference"
							},
							"name": "RecordNullRows"
						},
						{
							"dataset": {
								"referenceName": "MetaDataLogs",
								"type": "DatasetReference"
							},
							"name": "AddLogs"
						}
					],
					"transformations": [
						{
							"name": "CheckforDuplicates"
						},
						{
							"name": "CheckUpdate"
						},
						{
							"name": "MarkUpdates"
						},
						{
							"name": "TransformType"
						},
						{
							"name": "FilterExistingRows"
						},
						{
							"name": "SourceRowCount"
						},
						{
							"name": "DuplicateRowCount"
						},
						{
							"name": "ErrorRowCount"
						},
						{
							"name": "IgnoreRowCount"
						},
						{
							"name": "MergeDuplicateRows"
						},
						{
							"name": "MergeErrorRows"
						},
						{
							"name": "MergeIgnoreRows"
						},
						{
							"name": "FixNulls"
						},
						{
							"name": "AddSourceFile"
						}
					],
					"scriptLines": [
						"source(output(",
						"          postal_code as string,",
						"          state as string,",
						"          region as string,",
						"          country as string",
						"     ),",
						"     allowSchemaDrift: false,",
						"     validateSchema: true,",
						"     ignoreNoFilesFound: false) ~> ReadSource",
						"source(output(",
						"          Postal_Code as integer,",
						"          State as string,",
						"          Region as string,",
						"          Country_Region as string,",
						"          Country as string",
						"     ),",
						"     allowSchemaDrift: false,",
						"     validateSchema: true,",
						"     isolationLevel: 'READ_UNCOMMITTED',",
						"     format: 'table') ~> ReadSQL",
						"AddSourceFile aggregate(groupBy(postal_code,",
						"          state,",
						"          region,",
						"          country,",
						"          Source_File),",
						"     Dummy = sum(1)) ~> CheckforDuplicates",
						"TransformType, ReadSQL join(TransformType@postal_code == ReadSQL@Postal_Code,",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> CheckUpdate",
						"FilterExistingRows@Retain alterRow(updateIf(FilterExistingRows@Retain@state!=FilterExistingRows@Retain@State||FilterExistingRows@Retain@region!=FilterExistingRows@Retain@Region||FilterExistingRows@Retain@country!=FilterExistingRows@Retain@Country||FilterExistingRows@Retain@Country_Region!=FilterExistingRows@Retain@Country_Region)) ~> MarkUpdates",
						"CheckforDuplicates derive(postal_code = toInteger(postal_code),",
						"          Country_Region = case(region == 'East', 'Eastern US', case(region == 'West', 'Western US', case(region == 'Central', 'Central US', case(region == 'South', 'Southern US', ''))))) ~> TransformType",
						"CheckUpdate split((isNull(ReadSQL@Postal_Code) || (CheckforDuplicates@state != ReadSQL@State || CheckforDuplicates@region != ReadSQL@Region || CheckforDuplicates@country != ReadSQL@Country || TransformType@Country_Region != ReadSQL@Country_Region) )\r",
						"&& (!isNull(TransformType@postal_code) && !isNull(CheckforDuplicates@state) && !isNull(CheckforDuplicates@region) && !isNull(CheckforDuplicates@country) && !isNull(TransformType@Country_Region)),",
						"     (isNull(TransformType@postal_code) || isNull(CheckforDuplicates@state) || isNull(CheckforDuplicates@region) || isNull(CheckforDuplicates@country) || isNull(TransformType@Country_Region)),",
						"     disjoint: false) ~> FilterExistingRows@(Retain, FilterNullRows, Ignore)",
						"AddSourceFile aggregate(groupBy(Source_File),",
						"     SRowCount = sum(1)) ~> SourceRowCount",
						"CheckforDuplicates aggregate(groupBy(Source_File),",
						"     DuplicateRowCount = sum(iif(Dummy != 1, Dummy - 1, toLong(0)))) ~> DuplicateRowCount",
						"FilterExistingRows@FilterNullRows aggregate(groupBy(Source_File),",
						"     ERowcount = sum(1)) ~> ErrorRowCount",
						"FilterExistingRows@Ignore aggregate(groupBy(Source_File),",
						"     IRowCount = sum(1)) ~> IgnoreRowCount",
						"SourceRowCount, DuplicateRowCount join(SourceRowCount@Source_File == DuplicateRowCount@Source_File,",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> MergeDuplicateRows",
						"MergeDuplicateRows, ErrorRowCount join(SourceRowCount@Source_File == ErrorRowCount@Source_File,",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> MergeErrorRows",
						"MergeErrorRows, IgnoreRowCount join(SourceRowCount@Source_File == IgnoreRowCount@Source_File,",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> MergeIgnoreRows",
						"MergeIgnoreRows derive(DuplicateRowCount = iif(isNull(DuplicateRowCount),0,toInteger(DuplicateRowCount)),",
						"          ERowcount = iif(isNull(ERowcount),0,toInteger(ERowcount)),",
						"          IRowCount = iif(isNull(IRowCount),0,toInteger(IRowCount))) ~> FixNulls",
						"ReadSource derive(Source_File = 'PostalCode') ~> AddSourceFile",
						"MarkUpdates sink(allowSchemaDrift: false,",
						"     validateSchema: true,",
						"     input(",
						"          Postal_Code as integer,",
						"          State as string,",
						"          Region as string,",
						"          Country_Region as string,",
						"          Country as string",
						"     ),",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:true,",
						"     upsertable:false,",
						"     keys:['Postal_Code'],",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'allErrors',",
						"     outputRejectedData: true,",
						"     rejectedData_container: 'tesmetadata',",
						"     transactionCommit: 'single',",
						"     reportSuccessOnError: false,",
						"     mapColumn(",
						"          Postal_Code = FilterExistingRows@Retain@postal_code,",
						"          State = FilterExistingRows@Retain@state,",
						"          Region = FilterExistingRows@Retain@region,",
						"          Country_Region = FilterExistingRows@Retain@Country_Region,",
						"          Country = FilterExistingRows@Retain@country",
						"     )) ~> InsertinSQL",
						"FilterExistingRows@FilterNullRows sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> RecordNullRows",
						"FixNulls sink(allowSchemaDrift: false,",
						"     validateSchema: true,",
						"     input(",
						"          SOURCE_FILE as string,",
						"          SOURCE_ROW_COUNT as integer,",
						"          ERROR_ROWS as integer,",
						"          DUPLICATE_ROW_COUNT as integer,",
						"          IGNORE_ROW_COUNT as integer,",
						"          FINAL_COUNT as integer",
						"     ),",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     mapColumn(",
						"          SOURCE_FILE = SourceRowCount@Source_File,",
						"          SOURCE_ROW_COUNT = SRowCount,",
						"          ERROR_ROWS = ERowcount,",
						"          DUPLICATE_ROW_COUNT = DuplicateRowCount,",
						"          IGNORE_ROW_COUNT = IRowCount",
						"     )) ~> AddLogs"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/datasets/PostalCodesSQL')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/PostalCodesMetadata')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "PostalCodesExcel",
								"type": "DatasetReference"
							},
							"name": "ReadSource"
						},
						{
							"dataset": {
								"referenceName": "PostalCodesSQL",
								"type": "DatasetReference"
							},
							"name": "ReadSQL"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "PostalCodesSQL",
								"type": "DatasetReference"
							},
							"name": "InsertinSQL"
						}
					],
					"transformations": [
						{
							"name": "CheckforDuplicates"
						},
						{
							"name": "MergeSources"
						},
						{
							"name": "MarkUpdates"
						},
						{
							"name": "TransformType"
						},
						{
							"name": "FilterExistingRows"
						}
					],
					"scriptLines": [
						"source(output(",
						"          postal_code as string,",
						"          state as string,",
						"          region as string,",
						"          country as string",
						"     ),",
						"     allowSchemaDrift: false,",
						"     validateSchema: true,",
						"     ignoreNoFilesFound: false) ~> ReadSource",
						"source(output(",
						"          Postal_Code as integer,",
						"          State as string,",
						"          Region as string,",
						"          Country_Region as string,",
						"          Country as string",
						"     ),",
						"     allowSchemaDrift: false,",
						"     validateSchema: true,",
						"     isolationLevel: 'READ_UNCOMMITTED',",
						"     format: 'table') ~> ReadSQL",
						"ReadSource aggregate(groupBy(postal_code,",
						"          state,",
						"          region,",
						"          country),",
						"     Dummy = sum(1)) ~> CheckforDuplicates",
						"TransformType, ReadSQL join(TransformType@postal_code == ReadSQL@Postal_Code,",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> MergeSources",
						"FilterExistingRows@Retain alterRow(updateIf(FilterExistingRows@Retain@state!=FilterExistingRows@Retain@State||FilterExistingRows@Retain@region!=FilterExistingRows@Retain@Region||FilterExistingRows@Retain@country!=FilterExistingRows@Retain@Country||FilterExistingRows@Retain@Country_Region!=FilterExistingRows@Retain@Country_Region)) ~> MarkUpdates",
						"CheckforDuplicates derive(postal_code = toInteger(postal_code),",
						"          Country_Region = case(region == 'East', 'Eastern US', case(region == 'West', 'Western US', case(region == 'Central', 'Central US', case(region == 'South', 'Southern US', ''))))) ~> TransformType",
						"MergeSources split((TransformType@postal_code != ReadSQL@Postal_Code || CheckforDuplicates@state != ReadSQL@State || CheckforDuplicates@region != ReadSQL@Region || CheckforDuplicates@country != ReadSQL@Country || TransformType@Country_Region != ReadSQL@Country_Region) || (isNull(ReadSQL@Postal_Code) && isNull(ReadSQL@State) && isNull(ReadSQL@Region) && isNull(ReadSQL@Country)),",
						"     disjoint: false) ~> FilterExistingRows@(Retain, Ignore)",
						"MarkUpdates sink(allowSchemaDrift: false,",
						"     validateSchema: true,",
						"     input(",
						"          Postal_Code as integer,",
						"          State as string,",
						"          Region as string,",
						"          Country_Region as string,",
						"          Country as string",
						"     ),",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:true,",
						"     upsertable:false,",
						"     keys:['Postal_Code'],",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     mapColumn(",
						"          Postal_Code = FilterExistingRows@Retain@postal_code,",
						"          State = FilterExistingRows@Retain@state,",
						"          Region = FilterExistingRows@Retain@region,",
						"          Country = FilterExistingRows@Retain@country,",
						"          Country_Region = FilterExistingRows@Retain@Country_Region",
						"     )) ~> InsertinSQL"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/datasets/PostalCodesSQL')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/ProductTypesMetadata')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "ProductTypesExcel",
								"type": "DatasetReference"
							},
							"name": "ReadSource"
						},
						{
							"dataset": {
								"referenceName": "ProductTypesSQL",
								"type": "DatasetReference"
							},
							"name": "ReadSQL"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "ProductTypesSQL",
								"type": "DatasetReference"
							},
							"name": "InsertinSQL"
						}
					],
					"transformations": [
						{
							"name": "CheckforDuplicates"
						},
						{
							"name": "MergeSources"
						},
						{
							"name": "MarkUpdates"
						},
						{
							"name": "FilterExistingRows"
						}
					],
					"scriptLines": [
						"source(output(",
						"          product_type as string,",
						"          product_category as string",
						"     ),",
						"     allowSchemaDrift: false,",
						"     validateSchema: true,",
						"     ignoreNoFilesFound: false) ~> ReadSource",
						"source(output(",
						"          Product_Type as string,",
						"          Product_Category as string",
						"     ),",
						"     allowSchemaDrift: false,",
						"     validateSchema: true,",
						"     isolationLevel: 'READ_UNCOMMITTED',",
						"     format: 'table') ~> ReadSQL",
						"ReadSource aggregate(groupBy(product_type,",
						"          product_category),",
						"     Dummy = sum(1)) ~> CheckforDuplicates",
						"CheckforDuplicates, ReadSQL join(CheckforDuplicates@product_type == ReadSQL@Product_Type,",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> MergeSources",
						"FilterExistingRows@Retain alterRow(updateIf(FilterExistingRows@Retain@product_category!=FilterExistingRows@Retain@Product_Category)) ~> MarkUpdates",
						"MergeSources split((CheckforDuplicates@product_type != ReadSQL@Product_Type || CheckforDuplicates@product_category != ReadSQL@Product_Category) || (isNull(ReadSQL@Product_Type) && isNull(ReadSQL@Product_Category)),",
						"     disjoint: false) ~> FilterExistingRows@(Retain, Ignore)",
						"MarkUpdates sink(allowSchemaDrift: false,",
						"     validateSchema: true,",
						"     input(",
						"          Product_Type as string,",
						"          Product_Category as string",
						"     ),",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:true,",
						"     upsertable:false,",
						"     keys:['Product_Type'],",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     mapColumn(",
						"          Product_Type = FilterExistingRows@Retain@product_type,",
						"          Product_Category = FilterExistingRows@Retain@product_category",
						"     )) ~> InsertinSQL"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/datasets/ProductTypesExcel')]",
				"[concat(variables('factoryId'), '/datasets/ProductTypesSQL')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/ProductTypesMetadata_v2')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "ProductTypesExcel",
								"type": "DatasetReference"
							},
							"name": "ReadSource"
						},
						{
							"dataset": {
								"referenceName": "ProductTypesSQL",
								"type": "DatasetReference"
							},
							"name": "ReadSQL"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "ProductTypesSQL",
								"type": "DatasetReference"
							},
							"name": "InsertinSQL",
							"rejectedDataLinkedService": {
								"referenceName": "AzureBlobStorage1",
								"type": "LinkedServiceReference"
							}
						},
						{
							"dataset": {
								"referenceName": "DumpNullMetaRows",
								"type": "DatasetReference"
							},
							"name": "RecordNullRows"
						},
						{
							"dataset": {
								"referenceName": "MetaDataLogs",
								"type": "DatasetReference"
							},
							"name": "AddLogs"
						}
					],
					"transformations": [
						{
							"name": "CheckforDuplicates"
						},
						{
							"name": "CheckUpdate"
						},
						{
							"name": "MarkUpdates"
						},
						{
							"name": "FilterExistingRows"
						},
						{
							"name": "SourceRowCount"
						},
						{
							"name": "DuplicateRowCount"
						},
						{
							"name": "ErrorRowCount"
						},
						{
							"name": "IgnoreRowCount"
						},
						{
							"name": "MergeDuplicateRows"
						},
						{
							"name": "MergeErrorRows"
						},
						{
							"name": "MergeIgnoreRows"
						},
						{
							"name": "FixNulls"
						},
						{
							"name": "AddSourceFile"
						}
					],
					"scriptLines": [
						"source(output(",
						"          product_type as string,",
						"          product_category as string",
						"     ),",
						"     allowSchemaDrift: false,",
						"     validateSchema: true,",
						"     ignoreNoFilesFound: false) ~> ReadSource",
						"source(output(",
						"          Product_Type as string,",
						"          Product_Category as string",
						"     ),",
						"     allowSchemaDrift: false,",
						"     validateSchema: true,",
						"     isolationLevel: 'READ_UNCOMMITTED',",
						"     format: 'table') ~> ReadSQL",
						"AddSourceFile aggregate(groupBy(product_type,",
						"          product_category,",
						"          Source_File),",
						"     Dummy = sum(1)) ~> CheckforDuplicates",
						"CheckforDuplicates, ReadSQL join(CheckforDuplicates@product_type == ReadSQL@Product_Type,",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> CheckUpdate",
						"FilterExistingRows@Retain alterRow(updateIf(FilterExistingRows@Retain@product_category!=FilterExistingRows@Retain@Product_Category)) ~> MarkUpdates",
						"CheckUpdate split((isNull(ReadSQL@Product_Type) || CheckforDuplicates@product_category != CheckforDuplicates@product_category ) \r",
						"&& (!isNull(CheckforDuplicates@product_type) && !isNull(CheckforDuplicates@product_category)),",
						"     (isNull(CheckforDuplicates@product_type) || isNull(CheckforDuplicates@product_category) ),",
						"     disjoint: false) ~> FilterExistingRows@(Retain, FilterNullRows, Ignore)",
						"AddSourceFile aggregate(groupBy(Source_File),",
						"     SRowCount = sum(1)) ~> SourceRowCount",
						"CheckforDuplicates aggregate(groupBy(Source_File),",
						"     DuplicateRowCount = sum(iif(Dummy != 1, Dummy - 1, toLong(0)))) ~> DuplicateRowCount",
						"FilterExistingRows@FilterNullRows aggregate(groupBy(Source_File),",
						"     ERowcount = sum(1)) ~> ErrorRowCount",
						"FilterExistingRows@Ignore aggregate(groupBy(Source_File),",
						"     IRowCount = sum(1)) ~> IgnoreRowCount",
						"SourceRowCount, DuplicateRowCount join(SourceRowCount@Source_File == DuplicateRowCount@Source_File,",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> MergeDuplicateRows",
						"MergeDuplicateRows, ErrorRowCount join(SourceRowCount@Source_File == ErrorRowCount@Source_File,",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> MergeErrorRows",
						"MergeErrorRows, IgnoreRowCount join(SourceRowCount@Source_File == IgnoreRowCount@Source_File,",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> MergeIgnoreRows",
						"MergeIgnoreRows derive(DuplicateRowCount = iif(isNull(DuplicateRowCount),0,toInteger(DuplicateRowCount)),",
						"          ERowcount = iif(isNull(ERowcount),0,toInteger(ERowcount)),",
						"          IRowCount = iif(isNull(IRowCount),0,toInteger(IRowCount))) ~> FixNulls",
						"ReadSource derive(Source_File = 'ProductTypes') ~> AddSourceFile",
						"MarkUpdates sink(allowSchemaDrift: false,",
						"     validateSchema: true,",
						"     input(",
						"          Product_Type as string,",
						"          Product_Category as string",
						"     ),",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:true,",
						"     upsertable:false,",
						"     keys:['Product_Type'],",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'allErrors',",
						"     outputRejectedData: true,",
						"     rejectedData_container: 'tesmetadata',",
						"     transactionCommit: 'single',",
						"     reportSuccessOnError: false,",
						"     mapColumn(",
						"          Product_Type = FilterExistingRows@Retain@product_type,",
						"          Product_Category = FilterExistingRows@Retain@product_category",
						"     )) ~> InsertinSQL",
						"FilterExistingRows@FilterNullRows sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> RecordNullRows",
						"FixNulls sink(allowSchemaDrift: false,",
						"     validateSchema: true,",
						"     input(",
						"          SOURCE_FILE as string,",
						"          SOURCE_ROW_COUNT as integer,",
						"          ERROR_ROWS as integer,",
						"          DUPLICATE_ROW_COUNT as integer,",
						"          IGNORE_ROW_COUNT as integer,",
						"          FINAL_COUNT as integer",
						"     ),",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     mapColumn(",
						"          SOURCE_FILE = SourceRowCount@Source_File,",
						"          SOURCE_ROW_COUNT = SRowCount,",
						"          ERROR_ROWS = ERowcount,",
						"          DUPLICATE_ROW_COUNT = DuplicateRowCount,",
						"          IGNORE_ROW_COUNT = IRowCount",
						"     )) ~> AddLogs"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/datasets/ProductTypesExcel')]",
				"[concat(variables('factoryId'), '/datasets/ProductTypesSQL')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/ProductsMetadata')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "ProductsExcel",
								"type": "DatasetReference"
							},
							"name": "ReadSource"
						},
						{
							"dataset": {
								"referenceName": "ProductsSQL",
								"type": "DatasetReference"
							},
							"name": "ReadSQL"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "ProductsSQL",
								"type": "DatasetReference"
							},
							"name": "InsertinSQL"
						}
					],
					"transformations": [
						{
							"name": "CheckforDuplicates"
						},
						{
							"name": "MergeSources"
						},
						{
							"name": "MarkUpdates"
						},
						{
							"name": "FilterExistingRows"
						}
					],
					"scriptLines": [
						"source(output(",
						"          product_id as string,",
						"          product_name as string,",
						"          category as string",
						"     ),",
						"     allowSchemaDrift: false,",
						"     validateSchema: true,",
						"     ignoreNoFilesFound: false) ~> ReadSource",
						"source(output(",
						"          Product_ID as string,",
						"          Product_Name as string,",
						"          Category as string",
						"     ),",
						"     allowSchemaDrift: false,",
						"     validateSchema: true,",
						"     isolationLevel: 'READ_UNCOMMITTED',",
						"     format: 'table') ~> ReadSQL",
						"ReadSource aggregate(groupBy(product_id,",
						"          product_name,",
						"          category),",
						"     Dummy = sum(1)) ~> CheckforDuplicates",
						"CheckforDuplicates, ReadSQL join(CheckforDuplicates@product_id == ReadSQL@Product_ID,",
						"     joinType:'left',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> MergeSources",
						"FilterExistingRows@Retain alterRow(updateIf(FilterExistingRows@Retain@product_name!=FilterExistingRows@Retain@Product_Name||FilterExistingRows@Retain@category!=FilterExistingRows@Retain@Category)) ~> MarkUpdates",
						"MergeSources split((CheckforDuplicates@product_id != ReadSQL@Product_ID || CheckforDuplicates@product_name != ReadSQL@Product_Name || CheckforDuplicates@category != ReadSQL@Category) || (isNull(ReadSQL@Product_ID) && isNull(ReadSQL@Product_Name) && isNull(ReadSQL@Category)),",
						"     disjoint: false) ~> FilterExistingRows@(Retain, Ignore)",
						"MarkUpdates sink(allowSchemaDrift: false,",
						"     validateSchema: true,",
						"     input(",
						"          Product_ID as string,",
						"          Product_Name as string,",
						"          Category as string",
						"     ),",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:true,",
						"     upsertable:false,",
						"     keys:['Product_ID'],",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     mapColumn(",
						"          Product_ID = FilterExistingRows@Retain@product_id,",
						"          Product_Name = FilterExistingRows@Retain@product_name,",
						"          Category = FilterExistingRows@Retain@category",
						"     )) ~> InsertinSQL"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/datasets/ProductsExcel')]",
				"[concat(variables('factoryId'), '/datasets/ProductsSQL')]"
			]
		}
	]
}